\documentclass{report}

\usepackage{amsfonts, amsmath, amssymb, amsthm}
\usepackage[margin=1.0in]{geometry}
\usepackage{hyperref}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathrsfs}

\newcommand{\M}[2]{\mathbb{#1}^{#2}}
\newcommand{\twovector}[2]{\left[ \begin{array}{c} #1 \\ #2 \\ \end{array} \right]}
\newcommand{\threevector}[3]{\left[ \begin{array}{c} #1 \\ #2 \\ #3 \\ \end{array} \right]}
\newcommand{\fourvector}[4]{\left[ \begin{array}{c} #1 \\ #2 \\ #3 \\ #4 \\ \end{array} \right]}
\newcommand{\fivevector}[5]{\left[ \begin{array}{c} #1 \\ #2 \\ #3 \\ #4 \\ #5 \\ \end{array} \right]}
\newcommand{\sixvector}[6]{\left[ \begin{array}{c} #1 \\ #2 \\ #3 \\ #4 \\ #5 \\ #6 \\ \end{array} \right]}
\newcommand{\sevenvector}[7]{\left[ \begin{array}{c} #1 \\ #2 \\ #3 \\ #4 \\ #5 \\ #6 \\ #7 \\ \end{array} \right]}
\newcommand{\eightvector}[8]{\left[ \begin{array}{c} #1 \\ #2 \\ #3 \\ #4 \\ #5 \\ #6 \\ #7 \\ #8 \\ \end{array} \right]}
\newcommand{\ninevector}[9]{\left[ \begin{array}{c} #1 \\ #2 \\ #3 \\ #4 \\ #5 \\ #6 \\ #7 \\ #8 \\ #9 \\ \end{array} \right]}
\newcommand{\nbrack}[1]{\left( #1 \right)}
\newcommand{\bbrack}[1]{\left[ #1 \right]}
\newcommand{\cbrack}[1]{\left\lbrace #1 \right\rbrace}
\newcommand{\abrack}[1]{\left< #1 \right>}
\newcommand{\linebrack}[1]{\left| #1 \right|}
\newcommand{\twomatrix}[4]{\bbrack{
    \begin{array}{cc}
      #1 & #2 \\
      #3 & #4 \\
    \end{array}
  }
}
\newcommand{\threematrix}[9]{\bbrack{
    \begin{array}{ccc}
      #1 & #2 & #3 \\
      #4 & #5 & #6 \\
      #7 & #8 & #9 \\
    \end{array}
  }
}
\newcommand{\Lplc}[1]{\mathscr{L}\bbrack{ #1 } (s)}
\newcommand{\iLplc}[1]{\mathscr{L}^{-1}\bbrack{ #1 } (t)}
\newcommand{\Var}[1]{\text{Var} \bbrack{ #1 }}
\newcommand{\fvv}[1]{\text{E} \bbrack{ #1 }}
\newcommand{\Prob}[1]{\text{P} \bbrack{ #1 }}

\title{Innlevering 5}
\author{Jacob Oliver Bruun}
\date{\today}

\makeatletter
\let\inserttitle\@title
\let\insertauthor\@author
\makeatother

\pagestyle{fancy}
\chead{\insertauthor}
\lhead{\inserttitle}
\rhead{\today}

\parindent 0ex

\begin{document}
\section*{Oppg. 1}
\textbf{a)}
Har en Poisson-fordeling og definerer da
\begin{equation}
  \label{eq:1}
  L(\mathbf{x}, \lambda_{R}) = f(x_{1}, \lambda_{R}) \cdots f(x_{n}, \lambda_{R})
\end{equation}
med
\begin{equation}
  \label{eq:2}
  f(x_{i}, \lambda_{R}) = \frac{\lambda_{R}^{x_{i}}}{x_{i}!} e^{-\lambda_{R}}
\end{equation}
og finner så
\begin{equation}
  \label{eq:3}
  \frac{\partial \ln L}{\partial \lambda_{R}} = \frac{\partial}{\partial \lambda_{R}} \cbrack{ \ln \bbrack{ \frac{\lambda_{R}^{\sum_{i=1}^{n} x_{i}}}{\prod_{j=1}^{n}x_{j}!} e^{-n\lambda_{R}} } } = \frac{\partial}{\partial \lambda_{R}} \cbrack{ \ln \lambda_{R} \sum_{i=1}^{n}x_{i} - \ln \prod_{j=1}^{n}x_{j}! - n\lambda_{R}} = \frac{1}{\lambda_{R}} \sum_{i=1}^{n}x_{i} - n
\end{equation}
setter vi så likning \eqref{eq:3} lik 0 finner vi
\begin{equation}
  \label{eq:4}
  \widehat{\lambda}_{R} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
\end{equation}
ser også at $\widehat{\lambda}_{R} = \overline{X}$ og har da
\begin{equation}
  \label{eq:5}
  \fvv{\widehat{\lambda}_{R}} = \fvv{ \frac{1}{n} \sum_{i=1}^{n} x_{i} } = \frac{1}{n} \fvv{ \sum_{i=1}^{n} x_{i}} = \frac{1}{n} \cdot n\lambda_{R} = \lambda_{R}
\end{equation}
og ser så på
\begin{equation}
  \label{eq:6}
  \Var{\widehat{\lambda}_{R}} = \Var{\frac{1}{n} \sum_{i=1}^{n} x_{i}} = \frac{1}{n^{2}} \Var{\sum_{i=1}^{n} x_{i}} = \frac{1}{n^{2}} \cdot n\lambda_{R} = \frac{\lambda_{R}}{n}
\end{equation}
ved sentralgrenseteoremet er dette normalfordelt. \\

\textbf{b)}
Setter da opp hypotesene
\begin{equation}
  \label{eq:7}
  H_{0} : \lambda_{R} = 10, \;\;\;\; H_{1} : \lambda_{R} < 10
\end{equation}
og kan sette opp ved sentralgrenseteoremet
\begin{equation}
  \label{eq:8}
  Z = \frac{\widehat{\lambda}_{R} - \lambda_{R}}{\sqrt{\frac{\lambda_{R}}{n}}} \approx -1.34164
\end{equation}
for $H_{0}$ og kan så finne $p$-verdien
\begin{equation}
  \label{eq:9}
  p = \Prob{ Z \leq -1.34164 } = 0.0901
\end{equation}
og har dermed med $\alpha = 0.05$ at vi ikke forkaster hypotesen. \\

\textbf{c)}
Antar $\lambda_{R} = 9$ og fortsetter å operere med $\alpha = 0.05$ og vil da finne
\begin{equation}
  \label{eq:48}
  \Prob{ \text{forkast } H_{0} | \lambda_{R} = 9} = 0.9 \Rightarrow \Prob{ p < 0.05 | \lambda_{R} = 9} = 0.9
\end{equation}
som da gir $Z \leq -2.58$ og siden vi har
\begin{equation}
  \label{eq:50}
  \widehat{\lambda}_{R} = -2.58\sqrt{\frac{10}{n}} + 10
\end{equation}

\section*{Oppg. 2}
\textbf{a)}
Ser på $Y_{i} = ax_{i}(1-x_{i}) + \epsilon_{i}$ for $i = 1, 2, \dots, n$ og har
\begin{equation}
  \label{eq:14}
  \fvv{ Y_{i} | x_{i} } = ax_{i} (1-x_{i}), \;\; \Var{ Y_{i} | x_{i} } = \sigma_{0}^{2} = 0.025^{2}
\end{equation}
ser på sannsynlighetsmaksimeringsestimatoren for $a$ og har $Y_{i} \sim N(ax_{i} (1-x_{i}), \sigma^{2})$ og dermed
\begin{equation}
  \label{eq:15}
  L(a) = f(\mathbf{y};a) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma_{0}} \exp\cbrack{ -\frac{ \bbrack{ y_{i} - ax_{i} (1-x_{i}) }^{2} }{ 2\sigma_{0}^{2} } }
\end{equation}
og har dermed
\begin{equation}
  \label{eq:10}
  \ln L(a) = \sum_{i=1}^{n} \cbrack{ -\ln \sqrt{2\pi} - \ln \sigma_{0} - \frac{\bbrack{ y_{i} - ax_{i}(1-x_{i}) }^{2}}{2\sigma_{0}} }
\end{equation}
og videre
\begin{equation}
  \label{eq:16}
  \begin{split}
    \partial_{a}\ln L(a) = \frac{1}{\sigma_{0}}\sum_{i=1}^{n} \bbrack{ y_{i} - \widehat{a}x_{i}(1-x_{i}) } \bbrack{x_{i}(1-x_{i})} &= 0 \\
    \sum_{i=1}^{n}\bbrack{x_{i}(1-x_{i})}y_{i} &= \widehat{a}\sum_{i=1}^{n} \bbrack{ x_{i} (1-x_{i}) }^{2} \\
    \widehat{a} &= \frac{\sum_{i=1}^{n} x_{i}(1 - x_{i})y_{i} }{ \sum_{i=1}^{n} \bbrack{ x_{i} (1-x_{i}) }^{2} }
  \end{split}
\end{equation}

\textbf{b)}
Skriver om
\begin{equation}
  \label{eq:18}
  \widehat{a} = \sum_{i=1}^{n}  \frac{x_{i}(1 - x_{i})y_{i} }{ \sum_{j=1}^{n} \bbrack{ x_{j} (1-x_{j}) }^{2} } = \sum_{i=1}^{n} a_{i} y_{i}, \;\;\; a_{i} = \frac{x_{i}(1 - x_{i})}{ \sum_{j=1}^{n} \bbrack{ x_{j} (1-x_{j}) }^{2} }
\end{equation}
der $a_{i}$ er en konstant og kan videre se
\begin{equation}
  \label{eq:17}
  \fvv{\widehat{a}} = \fvv{ \sum_{i=1}^{n} a_{i} y_{i} } = \sum_{i=1}^{n} \fvv{ y_{i} a_{i} } = \sum_{i=1}^{n}ax_{i}(1-x_{i}) a_{i} = a \frac{ \sum_{i=1}^{n} \bbrack{ x_{i}(1-x_{i}) }^{2} }{ \sum_{j=1}^{n} \bbrack{ x_{j}(1-x_{j}) }^{2} } = a
\end{equation}
og ser videre på variansen
\begin{equation}
  \label{eq:19}
  \Var{ \widehat{a} } = \Var{ \sum_{i=1}^{n} a_{i} y_{i} } = \sum_{i=1}^{n} \Var{ a_{i} y_{i} } = \sum_{i=1}^{n} a_{i}^{2} \sigma_{0}^{2} = \frac{\sigma_{0}^{2} \sum_{i=1}^{n} \bbrack{x_{i}(1 - x_{i})}^{2}}{ \cbrack{ \sum_{j=1}^{n} \bbrack{ x_{j} (1-x_{j}) }^{2} }^{2} } = \frac{\sigma_{0}^{2}}{ \sum_{j=1}^{n} \bbrack{ x_{j}(1-x_{j}) }^{2} }
\end{equation}
og kan se at $\widehat{a}$ er normalfordelt da det er en lineærkombinasjon av uavhengige normalfordelinger. \\

\textbf{c)}
Setter opp hypotesetesten
\begin{equation}
  \label{eq:20}
  H_{0} : a = 0, \;\;\; H_{1} : a \neq 0
\end{equation}
og kan fra informasjonen sette opp
\begin{equation}
  \label{eq:21}
  Z = \frac{\widehat{a} - a}{\sqrt{\Var{a}}} \Rightarrow Z_{0} = \frac{\widehat{a}}{\sqrt{\Var{a}}}
\end{equation}
og har at
\begin{equation}
  \label{eq:22}
  \begin{split}
    \widehat{a} &= \frac{\sum_{i=1}^{9} x_{i}(1-x_{i})y_{i} }{ \sum_{j=1}^{9} \bbrack{ x_{j} (1-x_{j}) }^{2} } \approx -0.283528 \\
    \Var{a} &= \frac{\sigma_{0}^{2}}{\sum_{j=1}^{9} \bbrack{ x_{j}(1-x_{j}) }^{2}} \approx 0.0018752
  \end{split}
\end{equation}
og med $z_{\alpha / 2} = 1.960$
\begin{equation}
  \label{eq:24}
  Z_{0} = -6.54746 < -z_{\alpha / 2}
\end{equation}
og må dermed forkaste hypotesen.


\section*{Oppg. 3}
\textbf{a)}
Har den stokastiske variablen $Y_{i} = \beta x_{i} + \epsilon_{i}$ med $\epsilon_{i} \sim N(0, \sigma^{2})$ og kan se på differansen
\begin{equation}
  \label{eq:25}
  \delta = \sum_{i=1}^{n} (y_{i} - \widehat{\beta} x_{i})^{2}
\end{equation}
og vil finne $\widehat{\beta}$ slik at $\alpha$ blir minst mulig, og ser da på
\begin{equation}
  \label{eq:23}
  \begin{split}
    \partial_{\beta} \delta = -2 \sum_{i=1}^{n} x_{i}(y_{i} - \widehat{\beta}x_{i}) = 0 \Rightarrow \sum_{i=1}^{n} x_{i} y_{i} = \widehat{\beta} \sum_{i=1}^{n} x_{i}^{2}
  \end{split}
\end{equation}
og har dermed
\begin{equation}
  \label{eq:26}
  \widehat{\beta} = \frac{\sum_{i=1}^{11} x_{i}y_{i}}{\sum_{i=1}^{11}x_{i}^{2}}
\end{equation}
og ved å sette inn verdier får vi
\begin{equation}
  \label{eq:27}
  \beta = 0.0567
\end{equation}
ser videre også at
\begin{equation}
  \label{eq:28}
  \fvv{ \widehat{\beta} } = \fvv{ \frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^{2}} }
  = \frac{1}{\sum_{i=1}^{n}x_{i}^{2}} \sum_{i=1}^{n} \fvv{ x_{i} y_{i} }
  =  \frac{1}{\sum_{i=1}^{n}x_{i}^{2}} \sum_{i=1}^{n} x_{i} \fvv{y_{i}}
\end{equation}
og kan se at
\begin{equation}
  \label{eq:30}
  \fvv{ y_{i} } = \fvv{ \beta x_{i} } + \fvv{ \epsilon_{i} } = \beta x_{i}
\end{equation}
og kan dermed se
\begin{equation}
  \label{eq:31}
  \fvv{\widehat{\beta}} = \frac{\beta \sum_{i=1}^{n} x_{i}^{2}}{\sum_{i=1}^{n} x_{i}^{2}} = \beta
\end{equation}
ser videre
\begin{equation}
  \label{eq:32}
  \Var{\widehat{\beta}} = \frac{1}{\bbrack{\sum_{i=1}^{n} x_{i}^{2}}^{2}} \sum_{i=1}^{n} x_{i}^{2} \Var{y_{i}}
\end{equation}
og kan se at
\begin{equation}
  \label{eq:33}
  \Var{y_{i}} = \Var{\beta x_{i}} + \Var{\epsilon_{i}} = \sigma^{2}
\end{equation}
og dermed
\begin{equation}
  \label{eq:34}
  \Var{\widehat{\beta}} = \frac{\sigma^{2} \sum_{i=1}^{n} x_{i}^{2}}{\bbrack{\sum_{i=1}^{n} x_{i}^{2}}^{2}} = \frac{\sigma^{2}}{\sum_{i=1}^{n}x_{i}^{2}}
\end{equation}

\textbf{b)}
Ser først at
\begin{equation}
  \label{eq:52}
  y_{0} = \widehat{\beta}x_{0} = 51.03
\end{equation}
ser så på $Y_{0}-\widehat{Y}_{0}$ som er normalfordelt og har også $\fvv{ Y_{0} - \widehat{Y}_{0} } = 0$ og
\begin{equation}
  \label{eq:53}
  \Var{ Y_{0} - \widehat{Y}_{0} } = \Var{Y_{0}} + \Var{\widehat{\beta}x_{0}} = \sigma^{2} \nbrack{ 1 + \frac{x_{0}^{2}}{\sum_{i=1}^{n} x_{i}^{2}} }
\end{equation}
og kan se på standardiseringen
\begin{equation}
  \label{eq:54}
  Z = \frac{Y_{0} - \widehat{Y}_{0}}{\sqrt{ \sigma^{2}\nbrack{ 1 + \frac{x_{0}^{2}}{\sum_{i=1}^{n} x_{i}^{2}} } }} \sim N(0,1)
\end{equation}
men siden vi ikke kjenner $\sigma^{2}$ kan vi heller bruke
\begin{equation}
  \label{eq:55}
  T = \frac{Y_{0} - \widehat{Y}_{0}}{\sqrt{ S^{2}\nbrack{ 1 + \frac{x_{0}^{2}}{ \sum_{i=1}^{n} x_{i}^{2} } } }} \sim t_{n-1}, \;\;\; S^{2} = \frac{1}{n-1} \sum_{i=1}^{n} (y_{i} - \widehat{y}_{i})^{2}
\end{equation}
og kan dermed finne intervallet slik at
\begin{equation}
  \label{eq:56}
  \Prob{ -t_{n-1, \alpha/2} \leq T \leq t_{n-1, \alpha/2} } = 0.95
\end{equation}
med $\alpha = 0.05$ kan vi finne prediksjonsintervallet
\begin{equation}
  \label{eq:57}
  Y_{0} \in \bbrack{ \widehat{Y}_{0} - t_{10, 0.025} \sqrt{ S^{2} \nbrack{ 1 + \frac{x_{0}^{2}}{ \sum_{i=1}^{n} x_{i}^{2} } } }, \widehat{Y}_{0} + t_{10, 0.025} \sqrt{ S^{2} \nbrack{ 1 + \frac{x_{0}^{2}}{ \sum_{i=1}^{n} x_{i}^{2} } } } } = \bbrack{ 48.50, 53.56 }
\end{equation}


\end{document}
